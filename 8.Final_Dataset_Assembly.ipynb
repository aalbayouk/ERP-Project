{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Dataset Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the files\n",
    "reddit_comments_df = pd.read_csv('/Users/aalbayouk/Desktop/ERP/DataSets/reddit_comments_updated_SUBMISSION.csv')\n",
    "comment_emotion_df = pd.read_csv('/Users/aalbayouk/Desktop/ERP/DataSets/comments_with_emotion_labels_SUBMISSION.csv')\n",
    "post_emotion_df = pd.read_csv('//Users/aalbayouk/Desktop/ERP/DataSets/posts_with_emotion_labels_NEW.csv')\n",
    "media_type_df = pd.read_csv('/Users/aalbayouk/Desktop/ERP/DataSets/posts_media_classified_with_sources.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to add all the new columns back into the main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter reddit_comments_df to keep only rows where post_id exists in media_type_df\n",
    "reddit_comments_df = reddit_comments_df[reddit_comments_df['post_id'].isin(media_type_df['post_id'])]\n",
    "\n",
    "# 2. Add the 'category', 'sub_category', and 'source' columns from media_type_df to reddit_comments_df\n",
    "reddit_comments_df = reddit_comments_df.merge(\n",
    "    media_type_df[['post_id', 'category', 'sub_category', 'source']], \n",
    "    on='post_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 2. Add the emotion_label from post_emotion_df as post_emotion \n",
    "reddit_comments_df = reddit_comments_df.merge(\n",
    "    post_emotion_df[['post_id', 'emotion_label']], \n",
    "    on='post_id', \n",
    "    how='left'\n",
    ").rename(columns={'emotion_label': 'post_emotion'})\n",
    "\n",
    "# 4. Add the emotion_label from comment_emotion_df as comment_emotion \n",
    "reddit_comments_df = reddit_comments_df.merge(\n",
    "    comment_emotion_df[['comment_id', 'emotion_label']], \n",
    "    on='comment_id', \n",
    "    how='left'\n",
    ").rename(columns={'emotion_label': 'comment_emotion'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check to see if there are any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment_emotion    0\n",
      "post_emotion       0\n",
      "category           0\n",
      "sub_category       0\n",
      "source             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the number of NaN values in each of the new columns\n",
    "nan_counts = reddit_comments_df[['comment_emotion', 'post_emotion', 'category', 'sub_category', 'source']].isna().sum()\n",
    "\n",
    "# Display the result\n",
    "print(nan_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Final Dataset\n",
    "reddit_comments_df.to_csv('/Users/aalbayouk/Desktop/ERP/DataSets/Final_Dataset_SUBMISSION.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
