{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Media and Source Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we classify the media as it is an important part of our study. \n",
    "\n",
    "There are two main classifications: traditional and non-traditional\n",
    "\n",
    "Then we further divided them into sub-categories to possibly gain more insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "url_file_path= '{add directory}/post_id_with_url_extracted.csv'\n",
    "\n",
    "url_df = pd.read_csv(url_file_path)\n",
    "\n",
    "# Drop rows where the 'url' column has NaN values\n",
    "url_df = url_df.dropna(subset=['url'])\n",
    "\n",
    "# Extract domain names from the new URLs\n",
    "url_df['domain'] = url_df['url'].str.extract(r'https?://([^/]+)')\n",
    "\n",
    "# Define the classification function with specific cases for social media, video, and image domains\n",
    "def classify_url_with_gfycat(url):\n",
    "    social_media_domains = ['x.com', 'twitter.com', 'facebook.com', 'fb.com', 'instagram.com', 'tiktok.com']\n",
    "    video_domains = ['youtube.com', 'youtu.be', 'vimeo.com', 'dailymotion.com', 'gfycat.com', 'v.redd.it']\n",
    "    image_domains = ['i.reddit.com', 'imgur.com', 'redd.it', 'gyazo.com', 'flickr.com', 'tinypic.com']\n",
    "\n",
    "    if any(domain in url for domain in social_media_domains):\n",
    "        return 'social_media'\n",
    "    elif any(domain in url for domain in video_domains) or \\\n",
    "         any(ext in url for ext in ['.mp4', '.avi', '.mov', '.wmv', '.flv', '.mkv','.watch']):\n",
    "        return 'video'\n",
    "    elif any(domain in url for domain in image_domains) or \\\n",
    "         any(ext in url for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
    "        return 'image'\n",
    "    elif 'reddit.com' in url or 'forum' in url or '/r/' in url:\n",
    "        return 'discussion'\n",
    "    else:\n",
    "        return 'article'\n",
    "\n",
    "# Check for any NaN or non-string values in the 'url' column\n",
    "url_df = url_df.dropna(subset=['url'])\n",
    "url_df = url_df[url_df['url'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "# Apply the updated classification function\n",
    "url_df['sub_category'] = url_df['url'].apply(classify_url_with_gfycat)\n",
    "\n",
    "# Create 'category' column based on 'sub_category'\n",
    "url_df['category'] = url_df['sub_category'].apply(lambda x: 'traditional' if x == 'article' else 'nontraditional')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the domain, we can extract the sources from it. I just mapped the most frequent sources as that is enough for my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization function to ensure consistent formatting using the domain column\n",
    "def normalize_source_from_domain(domain):\n",
    "    domain = str(domain).lower()\n",
    "\n",
    "    # Remove lingering prefixes\n",
    "    domain = re.sub(r'^(www\\d*|english|en|m|m-en|new)\\.', '', domain)\n",
    "    \n",
    "    # Known mappings based on observed patterns\n",
    "    specific_mappings = {\n",
    "        'reddit.com': 'reddit',\n",
    "        'redd.it': 'reddit',\n",
    "        'thejc.com': 'thejewishchronicle',\n",
    "        'm.jpost.com': 'thejerusalempost',\n",
    "        'jpost.com': 'thejerusalempost',\n",
    "        'bbc.com': 'bbc',\n",
    "        'cnn.com': 'cnn',\n",
    "        'nbcnews.com': 'nbcnews',\n",
    "        'bloomberg.com': 'bloomberg',\n",
    "        'ynetnews.com': 'ynetnews',\n",
    "        'reuters.com': 'reuters',\n",
    "        'dailynews.com': 'dailynews',\n",
    "        'thestar.com': 'thestar',\n",
    "        'theguardian.com': 'theguardian',\n",
    "        'thehindu.com': 'thehindu',\n",
    "        'pravda.com.ua': 'pravda',\n",
    "        'navalnews.com': 'navalnews',\n",
    "        'islamabadpost.com.pk': 'islamabadpost',\n",
    "        'apnews.com': 'associatedpress',\n",
    "        '1lurer.am': 'lurer',\n",
    "        'iranintl.com': 'iraninternational',\n",
    "        'aje.io': 'aljazeera',\n",
    "        'allisrael.com': 'allisraelnews',\n",
    "        'timesofisrael.com': 'thetimesofisrael',\n",
    "        'elpais.com': 'elpais',\n",
    "        'nytimes.com': 'thenewyorktimes',\n",
    "        'straitstimes.com': 'thestraitstimes',\n",
    "        'armradio.am': 'armenpress',\n",
    "        'i24news.tv': 'inews',\n",
    "        'youtu.be': 'youtube',\n",
    "        'bhaskar.com': 'bhaskar',\n",
    "        'kiis.com.ua': 'com',\n",
    "        'sky.com': 'skynews',\n",
    "        'skynews.com.au': 'skynews',\n",
    "        'france24.com': 'france',\n",
    "        'kyodonews.net': 'kyodonews',\n",
    "        'peacekeeping.un.org': 'un',\n",
    "        'news.un.org': 'un',\n",
    "        'indiatimes.com': 'indiatimes',\n",
    "        'joins.com': 'joins',\n",
    "        '9news.com.au': 'com',\n",
    "        'aa.com.tr': 'com',\n",
    "        'wikipedia.org': 'wikipedia',\n",
    "        'thejewishindependent.com.au': 'com',\n",
    "        'imgur.com': 'imgur',\n",
    "        'al-monitor.com': 'almonitor',\n",
    "        'washingtonpost.com': 'thewashingtonpost',\n",
    "        'yahoo.com': 'yahoonews',\n",
    "        'jam-news.net': 'jamnews',\n",
    "        'united24media.com': 'unitedmedia',\n",
    "        'elhayat-life.com': 'elhayatlife',\n",
    "        'cebudailynews.inquirer.net': 'inquirer',\n",
    "        'images.dawn.com': 'dawn',\n",
    "        'the-afc.com': 'theafc',\n",
    "        'mongabay.com': 'mongabay',\n",
    "        'abc.net.au': 'abc',\n",
    "        'abcnews.go.com': 'abc',\n",
    "        'www.abc57.com': 'abc',\n",
    "        'abc13.com': 'abc',\n",
    "        'huffpost.com': 'huffingtonpost',\n",
    "        'i24news.tv': 'i24news',\n",
    "        'france24.com': 'france24',\n",
    "        'news18.com': 'news18',\n",
    "        'nbcrightnow.com': 'nbcnews',\n",
    "        'nbcnewyork.com': 'nbcnews',\n",
    "        'iol.co.za': 'iol',\n",
    "        'actblue.com': 'actblue',\n",
    "        'twitter.com': 'x',\n",
    "        'openstreetmap.org': 'openstreetmap',\n",
    "        'ynet.co.il': 'ynet',\n",
    "        'rbc.ua': 'rbc',\n",
    "        'cp24.com': 'cp24',\n",
    "        'www.cbsnews.com': 'cbc',\n",
    "        'fb.watch': 'facebook',\n",
    "        'usni.org': 'usni'\n",
    "    }\n",
    "\n",
    "    # If the domain matches a specific mapping, use that\n",
    "    for key, value in specific_mappings.items():\n",
    "        if key in domain:\n",
    "            return value\n",
    "    \n",
    "    # Otherwise, use the first part of the domain as the source\n",
    "    main_part = domain.split('.')[0]\n",
    "    \n",
    "    # Return the normalized source name\n",
    "    return main_part\n",
    "\n",
    "# Apply this function to the domain column\n",
    "url_df['source'] = url_df['domain'].apply(normalize_source_from_domain)\n",
    "\n",
    "# Fill NaN values in the 'source' column with 'reddit' since all NaN are reddit\n",
    "url_df['source'] = url_df['source'].fillna('reddit')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sources were misclassified because of their prefixes, so we have to add specific logic to classify them properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to correct specific misclassified source \n",
    "def fix_source_from_domain(row):\n",
    "    source = str(row['source']).lower()\n",
    "    domain = str(row['domain']).lower()\n",
    "\n",
    "    # List of specific suffixes that are misclassified as sources\n",
    "    suffixes = ['europa', 'mod', 'com', 'ac', 'net', 'org', 'or']\n",
    "\n",
    "    # Check if the source is one of the suffixes\n",
    "    if source in suffixes:\n",
    "        # Extract the correct source from the domain\n",
    "        parts = domain.split('.')\n",
    "        for i in range(len(parts)):\n",
    "            if any(parts[i].endswith(suffix) for suffix in suffixes):\n",
    "                return parts[i-1]  # Return the word before the suffix\n",
    "    \n",
    "    # If the source is correct, return it as is\n",
    "    return row['source']\n",
    "\n",
    "# Apply the function to the DataFrame to update the source column\n",
    "url_df['source'] = url_df.apply(fix_source_from_domain, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "url_df.to_csv('/{add directory}/posts_media_classified_with_sources.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
