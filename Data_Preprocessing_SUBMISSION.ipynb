{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ftfy\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text Data and Create Smaller Task Specific Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "reddit_comments = pd.read_csv('{add directory}/reddit_opinion_PSE_ISR_june.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Irrelevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to remove\n",
    "columns_to_remove = [\n",
    "    'author_name', 'controversiality', 'ups', 'downs',\n",
    "    'user_is_verified', 'user_account_created_time', 'user_awardee_karma',\n",
    "    'user_awarder_karma', 'user_link_karma', 'user_comment_karma',\n",
    "    'user_total_karma', 'post_upvote_ratio', 'post_thumbs_ups', \n",
    "    'post_total_awards_received'\n",
    "]\n",
    "\n",
    "# Removing the columns from the DataFrame\n",
    "reddit_comments = reddit_comments.drop(columns=columns_to_remove)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FTFY Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in text columns with an empty string to prevent errors in ftfy\n",
    "reddit_comments['self_text'] = reddit_comments['self_text'].fillna('')\n",
    "reddit_comments['post_title'] = reddit_comments['post_title'].fillna('')\n",
    "reddit_comments['post_self_text'] = reddit_comments['post_self_text'].fillna('')\n",
    "\n",
    "\n",
    "# Clean the text columns in the merged dataset using ftfy\n",
    "reddit_comments['self_text'] = reddit_comments['self_text'].apply(ftfy.fix_text)\n",
    "reddit_comments['post_title'] = reddit_comments['post_title'].apply(ftfy.fix_text)\n",
    "reddit_comments['post_self_text'] = reddit_comments['post_self_text'].apply(ftfy.fix_text)\n",
    "\n",
    "# Remove any lingering problematic characters after ftfy processing\n",
    "reddit_comments['self_text'] = reddit_comments['self_text'].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x))\n",
    "reddit_comments['post_title'] = reddit_comments['post_title'].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x))\n",
    "reddit_comments['post_self_text'] = reddit_comments['post_self_text'].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Posts Dataset\n",
    "unique_posts_df = reddit_comments[['post_id', 'post_title', 'post_self_text', 'subreddit']].drop_duplicates(subset=['post_id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Out Irrelevant Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize keywords related to the Israel-Palestine topic\n",
    "keywords = [\n",
    "    'Israel', 'Palestine', 'Gaza', 'West Bank', 'Hamas', 'IDF', 'Jerusalem', 'Zionist', 'Hezbollah',\n",
    "    'Middle East conflict', 'two-state solution', 'Intifada', 'settlements', 'Nakba', 'al-Aqsa', 'peace talks',\n",
    "    'Palestinian Authority', 'Netanyahu', 'Fatah', 'aipac', 'rafah', 'palestinian', 'zionism',\n",
    "    'jabalia', 'protest', 'protestor', 'knesset', 'occupation', 'Natenyahu'\n",
    "]\n",
    "\n",
    "# Convert keywords to lowercase for case-insensitive matching\n",
    "keywords = [keyword.lower() for keyword in keywords]\n",
    "\n",
    "# Function to check if any keyword is in a given text\n",
    "def contains_keywords(text, keywords):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    text = text.lower()\n",
    "    return any(keyword in text for keyword in keywords)\n",
    "\n",
    "# List of multi-topic subreddits to filter\n",
    "target_subreddits = [\n",
    "    'CrazyFuckingVideos', 'CombatFootage', 'PublicFreakout', 'worldnewsvideo', 'worldnews',\n",
    "    'NonCredibleDefense', 'NoahGetTheBoat', 'AbruptChaos', 'TerrifyingAsFuck', 'ActualPublicFreakouts'\n",
    "]\n",
    "\n",
    "# Filter the dataframe to include only the target subreddits\n",
    "target_subreddit_df = unique_posts_df[unique_posts_df['subreddit'].isin(target_subreddits)]\n",
    "\n",
    "# Apply the keyword search to these filtered posts\n",
    "target_subreddit_df['relevant'] = target_subreddit_df.apply(\n",
    "    lambda row: contains_keywords(row['post_title'], keywords) or contains_keywords(row['post_self_text'], keywords),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Separate the irrelevant posts\n",
    "irrelevant_posts = target_subreddit_df[~target_subreddit_df['relevant']].drop(columns=['relevant'])\n",
    "\n",
    "# All other posts (including those not in the target subreddits) are considered relevant\n",
    "relevant_posts = unique_posts_df.drop(irrelevant_posts.index)\n",
    "\n",
    "# Save the relevant and irrelevant datasets to separate CSV files\n",
    "relevant_posts.to_csv('/{add directory}/relevant_posts.csv', index=False) # The relevant posts dataset will be used for running the RoBERTa on posts\n",
    "irrelevant_posts.to_csv('{add directory}/irrelevant_posts.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Subdatasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the entries in the reddit_comments dataframe that have a post_id in the irrelevant posts dataframe\n",
    "reddit_comments_updated = reddit_comments[~reddit_comments['post_id'].isin(irrelevant_posts['post_id'])]\n",
    "reddit_comments_updated.to_csv('{add directory}/reddit_comments_updated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. URL Extraction Dataset: Include only unique post_id\n",
    "url_extraction_df = reddit_comments_updated[['post_id']].drop_duplicates()\n",
    "url_extraction_df.to_csv('{add directory}/url_extraction_dfs.csv', index=False)\n",
    "\n",
    "# 2. Unique Comments Dataset\n",
    "unique_comments_df = reddit_comments_updated[['comment_id', 'self_text']].drop_duplicates(subset=['comment_id'])\n",
    "unique_comments_df.to_csv('{add directory}/unique_comments_df.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
